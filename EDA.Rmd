---
title: "EDA"
author: "Nora Lee Reinhardt and May Norman"
date: "`r Sys.Date()`"
output: html_document
---

This datafile is about the success of math students based on a set of input variables. The binary predictors are school, sex, address, famsize, Pstatus, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic. The categorical predictors are Medu, Fedu, Mjob, Fjob, reason, guardian, traveltime, studytime. The numeric variables are failures, absences. The target variable is G4 which is a categorical variable to indicate a studentâ€™s success. There are 31 predictors and 273 values of each. There are no NAs.

```{r}
library(tidyverse)
library(tree)

mathscores <- read.csv("MathScores.csv")

view(head(mathscores))


```


```{r}
summary(mathscores)
```

First we analyzed the response variable. About 11.7% of the students in the dataset had a student success result of excellent.  9.2% of students had fail, 37% had needs improvement and 42.1% had satisfactory.
```{r}
summary(mathscores$age)

mathscores %>% 
  ggplot(aes(x = age)) + 
  geom_bar(fill = "purple") +
  ggtitle('Age of Students')
```
The age of students ranges from 15 to 22 and the distribution is right skewed. The median age being 17 and mean being 16.66
```{r}
summary(mathscores$failures)


mathscores %>% 
  ggplot(aes(x = failures)) + 
  geom_bar(fill = "hotpink") +
  ggtitle('Number of Failures')
```
The majority of students have 0 failures. Less than 50 students have one failure. Mean of 0.304, median of 0 

```{r}
summary(mathscores$absences)

mathscores %>% 
  ggplot(aes(x = absences)) + 
  geom_bar(fill = "limegreen") +
  ggtitle('Number of Absences')

```
This distribution of absences is positively skewed. There are several outliers with high numbers of absences but the most common number of absences is 0.
```{r}
table(mathscores$sex)

mathscores %>% 
  ggplot(aes(x = sex)) + 
  geom_bar(fill = "skyblue") +
  ggtitle('Number of Sexes')

```
There are more female students in the data set than males. About 53.8% of the students are female.
```{r}
table(mathscores$address)


mathscores %>% 
  ggplot(aes(x = address)) + 
  geom_bar(fill = "lavender") +
  ggtitle('Number of adress types')
```
There are more students with urban addresses than rural addresses. Only 20.1% of the students have rural addresses.
```{r}
table(mathscores$school)

mathscores %>% 
  ggplot(aes(x = school)) + 
  geom_bar(fill = "blue") +
  ggtitle('Number of school types')

```
The majority of students attend school A. Only about 11.4% of the students are from school B.
```{r}
table(mathscores$reason)

mathscores %>% 
  ggplot(aes(x = reason)) + 
  geom_bar(fill = "yellow") +
  ggtitle('Reasons for choosing school')

```
The most popular reason for choosing a school was the courses offered and the reputation and it being close to home had similar results to eachother.
```{r}
table(mathscores$activities)

mathscores %>% 
  ggplot(aes(x = activities)) + 
  geom_bar(fill = "red") +
  ggtitle('extracurricular activities?')
```
The number of students in activities is close to the number of students not in activities. There is one more student who is not in activities.
```{r}
table(mathscores$schoolsup)

mathscores %>% 
  ggplot(aes(x = schoolsup)) + 
  geom_bar(fill = "brown") +
  ggtitle('Extra School Support?')
```
The majority of students do not have addition educational support. 
```{r}
table(mathscores$famsup)

mathscores %>% 
  ggplot(aes(x = famsup)) + 
  geom_bar(fill = "magenta") +
  ggtitle('Family School Support?')
```
Most students have family educational support 178 do and 95 don't
```{r}
table(mathscores$paid)

mathscores %>% 
  ggplot(aes(x = paid)) + 
  geom_bar(fill = "cyan3") +
  ggtitle('Extra Paid Classes?')

```

A little less than half of the students have extra paid classes within the math subject area (126 yes, 147 no).
```{r}
table(mathscores$traveltime)

mathscores %>% 
  ggplot(aes(x = traveltime)) + 
  geom_bar(fill = "orange") +
  ggtitle('Travel Time:')
```

There are 4 categories 1 = less than 15 mins, 2 = 15 to 30 minutes, 3 = 30 to 60 minutes and 4 = more than an hour. The majority of students take less than 15 minutes to get to class and only a few take more than an hour
```{r}
table(mathscores$studytime)


mathscores %>% 
  ggplot(aes(x = studytime)) +
  geom_bar(fill = "yellowgreen") + 
  ggtitle('Weekly Study Time')
```
Weekly study time is also split into 4 categories: 1 = less than 2 hours, 2 = 2 to 5 hours, 3 = 5 to 10 hours, 4 = more than 10 hours.  Most of the students study between 2 and 5 hours but alot also study for less than 2
```{r}
table(mathscores$freetime)

mathscores %>% 
  ggplot(aes(x = freetime)) + 
  geom_bar(fill = "lightsalmon") +
  ggtitle('Free Time after School')
```

Lastly, free time after school is on a scale of 1 to 5 with 1 being very low amount of free time and 5 being very high amount of free time. About a third of students answered 3, meaning they have average free time after school. The least amount of people answered 1 and the most answered 3 this might not be accurate because we don't know the numbers behind them.

In conclusion, there are both numerical and categorical variable that could help us predict a student's success in math class and building the model to do this. We should start by building a tree with variables with the "important" variables. We would also make a full model with all the variables without over fitting. A nonparametirc model might be better using this data because some of the variables have non normal distributions or they have outliers. 







```{r}

#mathscores <- mathscores %>%  select(-ID)
mathscores$school = as.factor(mathscores$school)
mathscores$sex = as.factor(mathscores$sex)
mathscores$address = as.factor(mathscores$address)
mathscores$famsize = as.factor(mathscores$famsize)
mathscores$Pstatus = as.factor(mathscores$Pstatus)
mathscores$Mjob = as.factor(mathscores$Mjob)
mathscores$Fjob = as.factor(mathscores$Fjob)
mathscores$reason = as.factor(mathscores$reason)
mathscores$guardian = as.factor(mathscores$guardian)
mathscores$activities = as.factor(mathscores$activities)
mathscores$schoolsup = as.factor(mathscores$schoolsup)
mathscores$famsup = as.factor(mathscores$famsup)
mathscores$paid = as.factor(mathscores$paid)
mathscores$activities = as.factor(mathscores$activities)
mathscores$nursery = as.factor(mathscores$nursery)
mathscores$higher = as.factor(mathscores$higher)
mathscores$internet = as.factor(mathscores$internet)
mathscores$romantic = as.factor(mathscores$romantic)
mathscores$traveltime = as.factor(mathscores$traveltime)
mathscores$studytime = as.factor(mathscores$studytime)
mathscores$freetime = as.factor(mathscores$freetime)






math_idx = sample(1:nrow(mathscores),nrow(mathscores)*.7)
mathtrain <- mathscores[math_idx,]
mathtest <- mathscores[-math_idx,]

table(mathtest$G4)

table(mathtrain$G4)


mathtrain$G4 <- as.factor(mathtrain$G4)
mathtest$G4  <- as.factor(mathtest$G4)



math_tree1 <- tree(G4 ~ ., data = mathtrain)
summary(math_tree1)
plot(math_tree1)
text(math_tree1, pretty = 1)
```

```{r}
math_pred <- predict(math_tree1, newdata = mathtrain, type = "class")
math_cm <- table(Predicted = math_pred, Actual = mathtrain$G4)
train_acc <- mean(math_pred == mathtrain$G4)
math_cm
train_acc

testpred1 = predict(math_tree1, newdata = mathtest)

```
```{r}
library(tree)

# 1. Fit initial tree
math_tree1 <- tree(G4 ~ ., data = mathtrain)

# 2. Cross-validation for optimal size
set.seed(1)
cv_math <- cv.tree(math_tree1, FUN = prune.tree)

# Visualize CV results
plot(cv_math$size, cv_math$dev, type = "b",
     xlab = "Tree Size", ylab = "Deviance (CV Error)")

# 3. Prune to optimal size
best_size <- cv_math$size[which.min(cv_math$dev)]
pruned_math_tree <- prune.tree(math_tree1, best = best_size)

# 4. Plot pruned tree
plot(pruned_math_tree)
text(pruned_math_tree, pretty = 1)

predict1 = predict(pruned_math_tree, newdata = mathtest, type = "class")
table(Predicted = predict1,Actual = mathtest$G4)
```

```{r}
library(randomForest)

mathscoresrf <- randomForest(G4 ~ ., data = mathtrain)


mathscoresrf
plot(mathscoresrf)

plot(mathscoresrf$err.rate[, "OOB"], 
     type = "l", 
     xlab = "Number of Trees", 
     ylab = "OOB Error Rate", 
     main = "Out-of-Bag Error vs. Number of Trees")

#optimal oob i think is around 100 trees

mathscoresrf1 <- randomForest(G4 ~ ., data = mathtrain, ntree = 100)

mathscoresrf1

plot(mathscoresrf1)

str(mathscoresrf1)
varImpPlot(mathscoresrf1)

predictforest = predict(mathscoresrf1, newdata = mathtest, type = "class")
table(Predicted = predictforest, Actual = mathtest$G4)
```

